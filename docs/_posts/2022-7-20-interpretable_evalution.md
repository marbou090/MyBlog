---
title: 解釈可能性の評価について
tags: 
 - 論文読み
 - 機械学習
 - 解釈可能性/説明可能性
---

解釈可能性/説明可能性の評価方法についての資料があまりなかったため、サーベイから評価方法の部分についてまとめてみました。元が英語なため私の訳が不正確であるという可能性と、私自身が完全理解ができていないという点で、きちんと興味がある人は元論文を読むことをおすすめします。

もし間違った点、不正確である点などありましたら、私のTwitter[(@mr__py)](https://twitter.com/mr__py)までお願いします。

参考にしたサーベイ：[https://www.mdpi.com/2079-9292/8/8/832/htm](https://www.mdpi.com/2079-9292/8/8/832/htm)

---

# 解釈可能性の評価

まず、MLに関する解釈可能性の単一の定義がない。同じく解釈可能性の評価についてもそう。ただ、既存の研究でアプローチを策定する試みがある。

## 解釈可能性の評価の３つの主要なレベル

- アプリケーションに基づく評価
    
    実際のアプリケーション内でユーザーの実験によって評価する。ユーザーが使い、評価を行う。このとき、ユーザーはドメイン知識を持つ専門家である必要がある。適切なベースラインは、人間が同じ決定を説明するのにどれくらい優れているか。
    
- 人間による評価
    
    上との違いは、実験はドメインの専門家(MLに精通してるとか)ではない一般の人で行われることで。ドメイン知識が必要ないために、実験が安価。
    
- 機能的に根拠のある評価
    
    解釈可能性についての定義をある機能に置き換え、評価を行う。例えば、決定木の深さを解釈可能性の高さとし、深さで測るなど。
    

アプリケーションに基づく評価は、適切ではあるが、コストがかかる。また、異なるドメインでの結果と比較しにくい。機能的に根拠のある評価については、通常異なるドメインとも比較可能。ただこれも解釈可能性の定義の置き換えに過ぎず、人間のフィードバックがないために結果の妥当性が低くなる。人間に基づいた評価は中間的な解決策。

## 解釈可能性の目標

解釈可能性が目指す目標は定義する余地がある。

- 精度
    
    説明方法による与えられた説明とMLモデルが出す予測の間の関係。説明しようとしている予測への忠実さが高くないと、説明が役に立たなくなる。
    
- 理解しやすさ
    
    ユーザーが説明を理解する方法の簡単さ。説明が正確でも、人間がそれを理解できないといけない。
    
- 効率
    
    ユーザーが説明を理解するのに必要な時間。無限時間使えばほとんど全てのモデルが解釈可能だと言えてしまう。理解するのに費やせる時間は有限で、ある程度短い時間である必要がある。一般に、説明が理解しやすいほど、より効率的である。
    

まとめると、高い解釈可能性とは、データとモデルに対して正確で、平均的なユーザにとって理解可能で、短時間で把握できる説明によってスコア付けされる。ただ、通常これらの目標はトレードオフの関係がある。例えば、説明が正確であるほど、理解がしにくいなど。

# 解釈可能性の評価に関するアプローチ

MLの解釈可能性評価の手段やアプローチはあまり論じられていない。XAIについての論文のうち5%ほどが解釈可能性の方法を評価することについて論じている。

MLの解釈可能性評価に関する既存の研究は、主に解釈可能性の特性を測定しようとする指標に関係していて、異なる説明方法の特徴づけや区別を可能にする。

## 定性的解釈可能性の指標

定性的指標について、説明に関連する5つの要因に言及している。

認知的チャンクとは、説明の基本単位を指す。

- 認知的チャンクの形式
    
    説明が何から構成されているのかに関係する。例えば、Feature Importanceであったり、訓練セットの例であったり、ルールリストであったり。画像認識では画素のグループなど。
    
- 説明の中に含まれる認知的チャンクの数
    
    例えば、訓練セットの例を出すときに、特徴量よりも多くの情報が含まれる可能性があることを考慮し、理解のしやすさの観点から、特徴量を出すのと訓練セットの例を出すのを同じ程度の量で扱うことができるのか。また、説明が特徴量で構成されている場合、全ての特徴量を含むのかそれとも選択した一部のみにするのか。
    
- 構成性
    
    認知的チャンクの構成と構造に関係する。例えば、Feature Importanceの値の順序など。
    
- 単調性とユニット間の他の相互作用
    
    相互作用は線形や非線形、または単調である可能性がある。人間にとってどの関係がより直感的であるのか。
    
- 不確実性と確率性
    
    説明の際に理解しやすい不確実性の尺度を返すかどうか。また、説明の生成にランダムなプロセスが含まれるのかどうか。
    

## 定量的解釈可能性の指標

説明の質を数値で定量化する(代理指標)は、異なる説明を比較するのに役立つ。

### Sundararajan et al.

DNNの予測値を元の入力特徴量に帰属させるためのアプローチを作成した。DNNの説明が満たすべき公理は２つ。

- ある１つの特徴量のみが違い、他は全部同じであるような入力に対して異なる予測があるとき。この特徴量のFeature Importanceは０以外である必要がある。予測の違いはその特徴量の違いによって引き起こされたはずである。
    
    逆もしかりで、予測が一部の特徴量に依存しない場合は、その特徴量のFeature Importanceは０である必要がある。
    
- ２つのDNNが等しい時、つまり同じ予測タスクで同じデータセットで学習して、同じ入力で同じ予測を返す時、実装が異なっていても説明が同じである必要がある。

### Honegger

オブジェクトをそれに対応する説明に関連づけることで構成されている。「オブジェクト」はインスタンスのセットとそれに対応する予測を指す。「説明」はFeature Importanceの値を指す。

- アイデンティティ
    
    同一のオブジェクトには同一の説明が必要である。
    
- 分離可能性
    
    同一でないオブジェクトは同一の説明を持つことができない。
    
- 安定性
    
    類似したオブジェクトには類似した説明が必要である。入力データのわずかな変化で、予測がわずかに変化するとき、予測アルゴリズムは安定していると言われる。これと同様の発想。
    

### Wilson et al.

特定の仮定の元で、説明の質を要約する目的で二項分類で使用される３つの代理指標を定義した。このタスクで考慮される説明はルールベースと、例を出すものである。彼らは説明を「データのローカルコンテキストに適用できる単純なモデル」と見做し、以下を最大化することで適切な説明を定義した。

- 完全性
    
    説明によって構成される事例の数。説明の網羅性に関連している。
    
- 正しさ
    
    説明によってカバーされるインスタンスのラベルの一貫性。つまり、正しい説明によってカバーされるインスタンスは同じラベルを持つべきであるということ。
    
- コンパクト性
    
    説明は簡潔であるべきで、決定規則の条件数と近傍ベース説明の特徴次元によって検証できる。これは説明の選択性と関連している。
    

# 参考文献

[https://www.mdpi.com/2079-9292/8/8/832/htm](https://www.mdpi.com/2079-9292/8/8/832/htm)